alloy:
  extraPorts:
    - name: otel-otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP

  configMap:
    content: |-
      logging {
        level = "info"
        format = "logfmt"
      }

      // Discover all pods and nodes in the cluster
      discovery.kubernetes "pods" {
        role = "pod"
      }
      discovery.kubernetes "nodes" {
        role = "node"
      }

      // Relabel to filter pods that have prometheus annotations for metrics scraping
      discovery.relabel "metrics" {
        targets = discovery.kubernetes.pods.targets
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_port"]
          target_label  = "__meta_kubernetes_pod_container_port_number"
          action = "keepequal"         // keep pods where there's a port matching the prometheus scrape port annotation
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_port_number"]
          regex = ""
          action = "drop"             // drop any target that doesn't have a port (i.e., no prometheus_io_port annotation)
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_path"]
          target_label  = "__metrics_path__"
          action = "replace"
          separator = ""
        }
      }

      // Metrics scrape: scrape metrics from annotated pod endpoints
      prometheus.scrape "metrics" {
        targets    = discovery.relabel.metrics.output
        scrape_interval = "30s"
        honor_labels = true
        forward_to = [ prometheus.remote_write.default.receiver ]
      }

      // Scrape node (kubelet/cAdvisor) metrics for all nodes
      discovery.relabel "pods_metrics" {
        targets = discovery.kubernetes.nodes.targets
        rule {
          replacement  = "kubernetes.default.svc:443"
          target_label = "__address__"
        }
        rule {
          regex         = "(.+)"
          replacement   = "/api/v1/nodes/$1/proxy/metrics/cadvisor"
          source_labels = ["__meta_kubernetes_node_name"]
          target_label  = "__metrics_path__"
        }
      }
      prometheus.scrape "pods_metrics" {
        targets      = discovery.relabel.pods_metrics.output
        job_name     = "kubelet-cadvisor"
        scheme       = "https"
        honor_labels = true
        scrape_interval = "30s"
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        tls_config {
          insecure_skip_verify = true
          server_name = "kubernetes"
        }
        forward_to = [ prometheus.remote_write.default.receiver ]
      }

      // Collect host OS metrics (node OS metrics via built-in exporter)
      prometheus.exporter.unix "node_os" {}
      prometheus.scrape "os_metrics" {
        targets    = prometheus.exporter.unix.node_os.targets
        scrape_interval = "30s"
        forward_to = [ prometheus.remote_write.default.receiver ]
      }

      // Remote write exporter for metrics, sends to Grafana Mimir (Prometheus remote-write API)
      prometheus.remote_write "default" {
        endpoint {
          url = "http://{{ .Release.Name }}-mimir-nginx.monitoring.svc.cluster.local/api/v1/push"
        }
      }

      // Log collection: use Kubernetes integration to read pod logs
      loki.source.kubernetes "pods" {
        targets    = discovery.kubernetes.pods.targets
        forward_to = [ loki.process.batch.receiver ]
      }
      
      // Process logs: example pipeline to parse JSON logs and extract level
      loki.process "batch" {
        // Drop very old logs (already an hour old) - to avoid backlog on start
        stage.drop {
          older_than = "1h"
          drop_counter_reason = "too_old"
        }
        stage.json {
          expressions = { level = "level" }   // parse JSON field "level"
          // (Add other fields as needed)
        }
        stage.labels {
          values = { level = "level" }        // promote log level to a label
        }
        forward_to = [ loki.write.default.receiver ]
      }

      // Send logs to Loki
      loki.write "default" {
        endpoint {
          url = "http://{{ .Release.Name }}-loki-distributor.monitoring.svc.cluster.local:3100/loki/api/v1/push"
        }
      }

      // Trace collection: receive spans via OpenTelemetry (OTLP) and OpenCensus
      otelcol.receiver.otlp "default" {
        grpc {
          endpoint = "0.0.0.0:4317"
        }
        output { 
          traces = [
            otelcol.connector.servicegraph.default.input,
            otelcol.processor.attributes.default.input,
          ]
        }
      }

      otelcol.connector.servicegraph "default" {
        dimensions = ["http.method", "http.target"]
        output {
          metrics = [otelcol.exporter.otlp.default.input]
        }
      }

      // Add a resource attribute to traces (e.g., cluster name) for identification
      otelcol.processor.attributes "default" {
        action {
          key = "cluster_name"
          value = "homelab-cluster"
          action = "insert"
        }
        output { traces = [ otelcol.processor.batch.default.input ] }
      }

      otelcol.processor.batch "default" {
        timeout = "1s"
        output { traces = [ otelcol.exporter.otlp.default.input ] }
      }

      // Export traces to Tempo (via OTLP)
      otelcol.exporter.otlp "default" {
        client {
          endpoint = "{{ .Release.Name }}-tempo-distributor.monitoring.svc.cluster.local:4317"
          tls {
            insecure = true
            insecure_skip_verify = true
          }
        }
      }